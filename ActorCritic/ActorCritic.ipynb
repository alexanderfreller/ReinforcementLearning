{"cells":[{"cell_type":"code","metadata":{},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow.keras.layers as layers\n","import gym\n","import numpy as np\n","import random\n","import math"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["def buildActor():\n","    inputs = tf.keras.Input(shape=(8))\n","    x = layers.Dense(64,activation='relu')(inputs)\n","    x = layers.Dense(128,activation='relu')(x)\n","    x = layers.Dense(64,activation='relu')(x)\n","    x = layers.Dense(4)(x)\n","    return tf.keras.Model(inputs=inputs,outputs=x)"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["def buildCritic():\n","    inputs = tf.keras.Input(shape=(8))\n","    x = layers.Dense(32,activation='relu')(inputs)\n","    x = layers.Dense(64,activation='relu')(x)\n","    x = layers.Dense(32,activation='relu')(x)\n","    x = layers.Dense(1)(x)\n","\n","    return tf.keras.Model(inputs = inputs, outputs = x) "]},{"cell_type":"code","metadata":{},"outputs":[],"source":["env = gym.make('LunarLander-v2')\n","\n","actor = buildActor()\n","critic = buildCritic()\n","\n","actor_optimizer = tf.keras.optimizers.Adam()\n","critic_optimizer = tf.keras.optimizers.Adam()\n","critic_loss = tf.keras.losses.MeanSquaredError()"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["MAX_EPSILON = 1\n","MIN_EPSILON = 0.01\n","LAMBDA = 0.00005\n","GAMMA = 0.95\n","BS = 128\n","TAU = 0.08"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["step = 0"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["render = True"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["class Memory:\n","  def __init__(self):\n","    self.states = []\n","    self.actions = []\n","    self.rewards = []\n","    \n","  def store(self, state, action, reward):\n","    self.states.append(state)\n","    self.actions.append(action)\n","    self.rewards.append(reward)\n","    \n","  def clear(self):\n","    self.states = []\n","    self.actions = []\n","    self.rewards = []\n","        "]},{"cell_type":"code","metadata":{},"outputs":[],"source":["for episode in range(1000):\n","\n","    done = False\n","    state = np.array(env.reset())\n","    totalReward = 0\n","    loss_value = 0\n","\n","    while not done:\n","        if render: env.render()\n"," \n","        with tf.GradientTape(persistent=True) as t:\n","\n","            logits = actor(state[None,:])\n","            action = tf.random.categorical(logits,1,dtype=tf.int32).numpy()[0,0]\n","\n","            next_state,reward,done,info = env.step(action)\n","\n","            totalReward += reward\n","            target = reward + GAMMA * critic(next_state[None,:])\n","            value = critic(state[None,:])\n","            td_error = target - value\n","\n","            prob = tf.nn.softmax(logits)\n","            actor_loss = -tf.math.log(prob[0,action] + 1e-5) * tf.stop_gradient(td_error)\n","\n","            grads = t.gradient(actor_loss,actor.trainable_variables)\n","            actor_optimizer.apply_gradients(zip(grads,actor.trainable_variables))\n","\n","            loss_value = critic_loss(target,value)\n","            grads = t.gradient(loss_value,critic.trainable_variables)\n","            critic_optimizer.apply_gradients(zip(grads,critic.trainable_variables))\n","\n","        state = next_state\n","    print(episode,totalReward)\n","env.close()"]},{"cell_type":"code","metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.4"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}